# Story 2.3: Implement Client-Side Service Discovery

## Status
Draft

## Story
**As an** AegisSDK client,
**I want** to query the Registry Service to discover available instances of a target service
**so that** I always have an up-to-date list of healthy instances.

## Acceptance Criteria
1. Before an RPC call, the client queries the NATS KV Store for all instance keys of the target service.
2. The client uses the returned list to select an instance for the call.
3. The client implements a caching strategy to avoid querying on every single call.

## Tasks / Subtasks
- [ ] Task 1: Create ServiceDiscoveryPort interface (AC: 1, 2)
  - [ ] Define abstract interface in aegis_sdk/ports/service_discovery.py
  - [ ] Include methods for discovering service instances
  - [ ] Include methods for instance selection strategies
  - [ ] Write unit tests for interface contracts
- [ ] Task 2: Implement basic service discovery without caching (AC: 1)
  - [ ] Create BasicServiceDiscovery class in aegis_sdk/infrastructure/
  - [ ] Implement discover_instances() method using KVServiceRegistry
  - [ ] Filter results to only return healthy instances
  - [ ] Handle service not found scenarios gracefully
  - [ ] Write unit tests with mocked registry
- [ ] Task 3: Implement instance selection strategies (AC: 2)
  - [ ] Add round-robin selection strategy
  - [ ] Add random selection strategy
  - [ ] Add sticky/preferred instance selection
  - [ ] Make strategy configurable per service
  - [ ] Write unit tests for each strategy
- [ ] Task 4: Implement caching layer for service discovery (AC: 3)
  - [ ] Create CachedServiceDiscovery wrapper class
  - [ ] Implement TTL-based local cache for discovery results
  - [ ] Add configurable cache TTL (default 10 seconds)
  - [ ] Implement cache invalidation on discovery failures
  - [ ] Add metrics for cache hits/misses
  - [ ] Write unit tests for caching behavior
- [ ] Task 5: Integrate discovery into Service base class RPC calls (AC: 1, 2)
  - [ ] Modify Service.call_rpc() to use service discovery
  - [ ] Add discovery_enabled parameter (default True)
  - [ ] Maintain backward compatibility with direct addressing
  - [ ] Handle discovery failures with fallback behavior
  - [ ] Update existing RPC tests
- [ ] Task 6: Add real-time cache updates via KV Watch (AC: 3)
  - [ ] Implement optional KV Store watch for service changes
  - [ ] Update cache when instances are added/removed
  - [ ] Handle watch connection failures gracefully
  - [ ] Make real-time updates opt-in via configuration
  - [ ] Write integration tests for watch functionality
- [ ] Task 7: Write comprehensive integration tests (AC: 1, 2, 3)
  - [ ] Test full discovery flow with K8s NATS cluster
  - [ ] Test multiple service instances registration and discovery
  - [ ] Test cache behavior under various scenarios
  - [ ] Test failover when instances become unhealthy
  - [ ] Test concurrent discovery requests
  - [ ] Achieve 100% coverage for discovery code

## Dev Notes

### Previous Story Insights
From Story 2.2 implementation:
- ServiceRegistryPort and KVServiceRegistry already implemented
- ServiceInstance model with health status and validation complete
- Registry key pattern: `service-instances.{serviceName}.{instanceId}`
- NATS KV Store integration available with TTL support
- Service base class has registration and heartbeat mechanisms
- K8s test environment available via `make dev-update` and `make port-forward`

### Data Models
ServiceInstance model already exists [Source: architecture/data-models.md#ServiceInstance]:
```python
class ServiceInstance(BaseModel):
    service_name: str
    instance_id: str
    version: str
    status: Literal["ACTIVE", "UNHEALTHY", "STANDBY"]
    last_heartbeat: datetime
    sticky_active_group: str | None = None
    metadata: dict[str, Any] = Field(default_factory=dict)
```

Available domain methods:
- `is_healthy()` - Returns True if status != "UNHEALTHY"
- `is_active()` - Returns True if status == "ACTIVE"
- `is_stale(threshold_seconds: int = 60)` - Check heartbeat freshness
- `seconds_since_heartbeat()` - Time since last heartbeat

### API Specifications
Service Registry methods available [Source: existing implementation]:
```python
# From ServiceRegistryPort
async def list_instances(self, service_name: str) -> list[ServiceInstance]
async def list_all_services(self) -> dict[str, list[ServiceInstance]]
async def get_instance(self, service_name: str, instance_id: str) -> ServiceInstance | None
```

RPC method signature to enhance [Source: Service base class]:
```python
async def call_rpc(self, service: str, method: str, params: dict[str, Any] | None = None) -> Any
```

### Component Specifications
Service Discovery Architecture [Source: inferred from patterns]:
- ServiceDiscoveryPort: Abstract interface for discovery operations
- BasicServiceDiscovery: Simple implementation without caching
- CachedServiceDiscovery: Wrapper adding caching capabilities
- Integration point: Service.call_rpc() method enhancement

### File Locations
Based on project structure [Source: architecture/unified-project-structure.md]:
```
/packages/aegis-sdk/
├── aegis_sdk/
│   ├── application/
│   │   └── service.py                    # Modify: Add discovery to call_rpc
│   ├── domain/
│   │   └── models.py                     # Existing: ServiceInstance model
│   ├── infrastructure/
│   │   ├── basic_service_discovery.py    # New: Basic discovery implementation
│   │   ├── cached_service_discovery.py   # New: Cached discovery wrapper
│   │   └── kv_service_registry.py        # Existing: Registry operations
│   └── ports/
│       └── service_discovery.py          # New: Discovery port interface
├── tests/
│   ├── unit/
│   │   ├── application/
│   │   │   └── test_service.py          # Modify: Test discovery integration
│   │   └── infrastructure/
│   │       ├── test_basic_service_discovery.py  # New: Basic discovery tests
│   │       └── test_cached_service_discovery.py # New: Cache tests
│   └── integration/
│       └── test_service_discovery_integration.py # New: E2E discovery tests
```

### Technical Constraints
- Python 3.13+ required [Source: architecture/coding-standards.md]
- Type annotations: 100% coverage with `from __future__ import annotations`
- Async/await for all I/O operations
- Pydantic v2 for data validation (no @dataclass)
- No mocking of NATS in integration tests - use K8s cluster

### Caching Implementation Details
Suggested caching approach:
- Default cache TTL: 10 seconds (configurable)
- Cache key format: `discovery:{service_name}`
- Cache invalidation triggers:
  - TTL expiration
  - RPC call failure to discovered instance
  - Manual refresh request
- Metrics to track:
  - Cache hit rate
  - Discovery latency
  - Stale instance detection

### Instance Selection Strategies
1. **Round-Robin**: Cycle through healthy instances
   - Maintain per-service counter
   - Skip unhealthy instances
   - Reset counter on instance list change

2. **Random**: Random selection from healthy instances
   - Use secrets.SystemRandom for selection
   - Equal probability distribution

3. **Sticky/Preferred**: Prefer specific instance if healthy
   - Check preferred instance health first
   - Fallback to round-robin if unhealthy
   - Useful for session affinity

### Error Handling Considerations
- Service not found: Return empty list, log warning
- All instances unhealthy: Raise ServiceUnavailableError
- KV Store connection failure: Use cached data if available
- Cache miss with KV failure: Raise DiscoveryError
- Instance becomes unhealthy during RPC: Retry with different instance

### Testing Requirements

**Test Framework**: Pytest (mandatory) [Source: architecture/testing-strategy.md]
- Use fixtures for test setup
- Use `@pytest.mark.asyncio` for async tests
- Use `@pytest.mark.parametrize` for test variations

**Integration Test Requirements**:
- MANDATORY: Use K8s environment from Story 0.2
- Deploy test cluster: `make dev-update`
- Port forward NATS: `make port-forward`
- Connect to: `nats://localhost:4222`

**Test Scenarios**:
1. **Discovery Tests**:
   - Discover instances for existing service
   - Handle non-existent service gracefully
   - Filter unhealthy instances correctly
   - Discover across multiple services

2. **Caching Tests**:
   - Cache hit within TTL window
   - Cache miss after TTL expiration
   - Cache invalidation on failure
   - Concurrent cache access

3. **Selection Strategy Tests**:
   - Round-robin cycles correctly
   - Random distribution is uniform
   - Sticky preference works when healthy
   - Fallback behavior on unhealthy preferred

4. **Integration Tests**:
   - Full RPC call with discovery
   - Failover on instance failure
   - Performance with many instances
   - Watch-based cache updates

**Coverage Requirements**:
- Minimum 80% overall
- 100% for discovery and caching logic

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-03 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
