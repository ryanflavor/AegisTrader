# Story 2.4: Integration Showcase

## Status
Approved

## Story
**As a** development team,
**I want** to demonstrate all implemented features working together in a real Kubernetes environment,
**so that** we can validate the complete system integration before implementing Epic 3.

## Acceptance Criteria
1. Deploy a complete working system to Kubernetes with all Epic 0-2 features functioning.
2. Demonstrate at least 3 different service instances registering and maintaining heartbeats.
3. Show the Management API successfully performing CRUD operations on service definitions.
4. Demonstrate service discovery working with multiple instances and automatic failover.
5. Show real-time monitoring of all services through the Monitor UI.
6. Validate performance metrics meet NFR requirements (p99 latency not degraded >10%).
7. Document the complete deployment process and provide demo scripts.

## Tasks / Subtasks
- [ ] Task 1: Create vnpy-based trading services (AC: 2, 4)
  - [ ] Create MarketDataService using vnpy for real-time market data distribution
  - [ ] Create TradingService using vnpy for order execution with single-active pattern
  - [ ] Create AlgoService using vnpy strategies for algorithmic trading
  - [ ] Implement vnpy event to SDK event adapters for seamless integration
  - [ ] Configure services with different instance counts (MarketData: 3, Trading: 2, Algo: 2)
  - [ ] Add vnpy-specific metrics (tick latency, order fill rate, strategy performance)
  - [ ] Write unit tests with vnpy mock environments
  - [ ] Create shared-contracts package with vnpy-compatible event schemas
  - [ ] Implement integration tests simulating real trading scenarios
- [x] Task 2: Enhance Helm charts for multi-service deployment (AC: 1)
  - [x] Create Helm templates for example services
  - [x] Add values configuration for service scaling
  - [x] Configure resource limits and requests
  - [x] Add service-specific environment variables
  - [x] Test Helm deployment locally with Kind cluster
  - [x] Fix SERVICE_TYPE environment variable case mismatch
  - [x] Disable health checks for services without HTTP endpoints
  - [x] Configure single instance per pod to avoid consumer conflicts
- [x] Task 3: Create integration test suite (AC: 2, 3, 4)
  - [x] Write K8s integration tests for service registration
  - [x] Test Management API CRUD operations in K8s environment
  - [x] Test service discovery with multiple instances
  - [x] Test failover scenarios (kill pods, simulate unhealthy)
  - [x] Verify automatic pod recreation and service re-registration
  - [x] Validate cache behavior under load
- [x] Task 4: Implement Monitor UI enhancements (AC: 5)
  - [x] Create dashboard with ServiceInstanceCard grid layout (as per wireframes-mockups.md#5.1)
  - [x] Implement ServiceStatusBadge component with semantic colors (ACTIVE=green, UNHEALTHY=red, STANDBY=gray)
  - [x] Create ServiceInstanceCard component showing service name, instance ID, status, and last heartbeat
  - [x] Add click navigation from cards to Service Detail View (/services/{instanceId})
  - [x] Implement auto-refresh using polling (5-second interval for real-time updates)
  - [x] Apply 8px-based grid system and Inter font for consistency
- [ ] Task 5: Create performance validation suite (AC: 6)
  - [ ] Run baseline performance tests from Story 1.2
  - [ ] Execute tests with full service registry active
  - [ ] Compare p99 latencies (must not degrade >10%)
  - [ ] Test with high service count (50+ instances)
  - [ ] Document performance results
- [ ] Task 6: Create demo scripts and documentation (AC: 7)
  - [ ] Write step-by-step deployment guide
  - [ ] Create demo script showing all features including:
    - User Flow 1: Monitor Service Health & Investigate Issues [Source: uiux/user-flows.md#4.1]
    - User Flow 2: Manage Service Definitions (CRUD) [Source: uiux/user-flows.md#4.2]
  - [ ] Record terminal session or create video demo
  - [ ] Document troubleshooting common issues
  - [ ] Create architecture diagram of deployed system

## Dev Notes

### Current K8s Environment Status
As of development start:
- Kind cluster running (aegis-local-control-plane, v1.27.3)
- NATS cluster: 3 nodes running with JetStream enabled
- Monitor API: Running on ClusterIP 10.96.120.184:8100
- Monitor UI: Running on ClusterIP 10.96.164.150:3100
- NATS Box: Available for CLI operations
- Makefile commands available for deployment automation

### Previous Story Insights
From Story 2.3 implementation:
- Service discovery with caching is fully implemented
- KV Store watch functionality enables real-time updates
- Multiple selection strategies available (round-robin, random, sticky)
- Integration tests already validate K8s environment connectivity

From Story 2.2 implementation:
- Service registration with heartbeats working
- ServiceInstance model includes health status tracking
- Registry uses key pattern: `service-instances.{serviceName}.{instanceId}`

From Story 2.1 implementation:
- Management API with CRUD operations complete
- FastAPI endpoints secured and functional
- NATS KV Store integration tested

### Data Models
ServiceInstance model available [Source: Story 2.2 Dev Notes]:
```python
class ServiceInstance(BaseModel):
    service_name: str
    instance_id: str
    version: str
    status: Literal["ACTIVE", "UNHEALTHY", "STANDBY"]
    last_heartbeat: datetime
    sticky_active_group: str | None = None
    metadata: dict[str, Any] = Field(default_factory=dict)
```

ServiceDefinition model [Source: Story 2.1 implementation]:
- Stored in Management API domain models
- Used for CRUD operations via REST API

### API Specifications
Management API endpoints [Source: Story 2.1]:
- `POST /api/services` - Create service definition
- `GET /api/services` - List all services
- `GET /api/services/{service_name}` - Get specific service
- `PUT /api/services/{service_name}` - Update service
- `DELETE /api/services/{service_name}` - Delete service

Service Discovery API [Source: Story 2.3]:
- Integrated into Service.call_rpc() method
- Automatic discovery with caching
- Configurable selection strategies

### Component Specifications
Monitor UI requirements [Source: docs/uiux/*]:
- Next.js application at `/apps/monitor-ui/`
- Dashboard layout with service instance cards [Source: uiux/wireframes-mockups.md#5.1]
- Service Detail View with tabbed interface [Source: uiux/wireframes-mockups.md#5.2]
- Use Shadcn/ui components with Tailwind CSS [Source: uiux/component-library-design-system.md]
- Real-time updates for operational efficiency [Source: uiux/overall-ux-goals-principles.md#2.2]

UI Component Specifications [Source: uiux/component-library-design-system.md]:
- **ServiceStatusBadge**: Colored badge for ACTIVE (green), UNHEALTHY (red), STANDBY (gray)
- **ServiceInstanceCard**: Dashboard component showing service instance summary
- **MetricsChart**: Visualize historical performance data
- **DefinitionForm**: Form for CRUD operations on service definitions
- **ConfirmDeleteDialog**: Modal for destructive action confirmation

Design Principles [Source: uiux/overall-ux-goals-principles.md#2.3]:
- Clarity Over Cleverness - prioritize clear communication
- Progressive Disclosure - show only what's needed
- Consistent Patterns - use familiar UI patterns
- Immediate Feedback - clear response to every action

Visual Design [Source: uiux/branding-style-guide.md]:
- Color Palette: Neutral grayscale, primary blue for interactive, semantic colors
- Typography: Modern sans-serif (Inter or system UI fonts)
- Spacing: 8px-based grid system for consistency

### File Locations
Based on project structure [Source: architecture/source-tree.md]:
```
/apps/
├── monitor-api/          # Management API (FastAPI)
├── monitor-ui/           # Monitoring UI (Next.js)
└── trading-service/      # Example services go here

/packages/
├── aegis-sdk/           # Core SDK with all features
└── shared-contracts/    # Shared type definitions

/helm/                   # Helm charts for deployment
├── templates/
└── values.yaml
```

### Technical Constraints
- Python 3.13+ required [Source: architecture/coding-standards.md]
- 100% type annotation coverage mandatory
- Pydantic v2 for all data models
- Async/await for all I/O operations
- Pytest framework for testing [Source: architecture/testing-strategy.md]

### Testing Requirements
Testing standards [Source: architecture/testing-strategy.md]:
- Test file location: tests/ directory in each package/app
- Minimum 80% coverage overall, 100% for critical paths
- Use pytest with async support (@pytest.mark.asyncio)
- Integration tests must use real NATS (no mocking)
- Test naming: test_{functionality}_{expected_behavior}

K8s testing approach:
- Use existing Kind cluster (aegis-local)
- Deploy with: `make dev-deploy` or `make dev-update`
- Port forward: `make dev-forward` or `make port-forward`
- Connect to: `nats://localhost:4222`
- Current deployment: 3-node NATS cluster, monitor-api, monitor-ui
- Namespace: aegis-trader

### Course Correction Notes
Task 1 being reimplemented with vnpy-based services to better align with real-world trading requirements.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-03 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-08-03 | 1.1 | Added UI/UX specifications from docs/uiux | Bob (Scrum Master) |
| 2025-08-05 | 1.2 | Task 1 reset: Switching to vnpy-based services | Bob (Scrum Master) |
| 2025-08-04 | 1.5 | Completed Task 2: Enhanced Helm charts and deployed all services | James (Dev Agent) |
| 2025-08-04 | 1.6 | Fixed deployment issues and optimized Makefile for rapid development | James (Dev Agent) |
| 2025-08-04 | 1.7 | Completed Task 3: Integration tests and fixed Management API CRUD bug | James (Dev Agent) |
| 2025-08-04 | 1.8 | Completed Task 4: Monitor UI enhancements with dashboard and service views | James (Dev Agent) |
| 2025-08-04 | 1.9 | Completed Task 3 failover tests: Verified K8s pod recovery and re-registration | James (Dev Agent) |
| 2025-08-04 | 2.0 | Fixed TypeScript build error in Monitor UI service type checking | James (Dev Agent) |
| 2025-08-04 | 2.1 | Fixed Monitor API and UI integration issues for service instance display | James (Dev Agent) |
| 2025-08-04 | 2.1 | Completed Task 3 failover testing with hexagonal architecture | James (Dev Agent) |

## Dev Agent Record

Task 1 reset and pending reimplementation with vnpy-based services.

## QA Results

Pending - will be updated after vnpy services implementation.

## Summary

Task 1 is being reimplemented to use vnpy-based trading services for better real-world alignment. Tasks 2-4 remain valid and completed.
