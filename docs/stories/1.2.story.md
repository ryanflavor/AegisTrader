# Story 1.2: Key Communication Patterns Performance Benchmarking

## Status
Draft

## Story
**As a** system architect,
**I want** to establish and document the baseline performance for the core RPC and Event communication patterns
**so that** we can objectively measure the performance impact of new features.

## Acceptance Criteria
1. A repeatable performance test script is created.
2. The p99 latency and max throughput (requests/sec) for the RPC pattern are measured and recorded.
3. The publish latency and max throughput (events/sec) for the JetStream Event pattern are measured and recorded.

## Tasks / Subtasks
- [ ] Task 1: Verify existing performance testing infrastructure (AC: 1)
  - [ ] Check if `packages/aegis-sdk/tests/integration/test_performance_benchmarks.py` exists
  - [ ] Check if `packages/aegis-sdk/tests/integration/test_performance_k8s.py` exists
  - [ ] Verify `packages/aegis-sdk/run_performance_benchmarks.py` script exists
  - [ ] Confirm pytest-benchmark and psutil dependencies are installed in pyproject.toml
  - [ ] Verify port-forwarding is active: `ps aux | grep "kubectl port-forward.*4222"`
  - [ ] Test NATS connectivity: `nc -zv localhost 4222`
- [ ] Task 2: Review and validate RPC pattern benchmarks (AC: 1, 2)
  - [ ] Review `test_performance_k8s.py::test_rpc_latency_benchmark` implementation
  - [ ] Verify it measures p50, p95, p99 latencies
  - [ ] Confirm warmup phase (100 calls) and test phase (1000 calls)
  - [ ] Check that MessagePack serialization is used
  - [ ] Verify connection pool size is set to 3
  - [ ] Run the RPC benchmark: `pytest tests/integration/test_performance_k8s.py::TestPerformanceBenchmarksK8s::test_rpc_latency_benchmark -v`
  - [ ] Confirm p99 latency is recorded and meets threshold (<5ms for port-forwarded connection)
- [ ] Task 3: Review and validate Event pattern benchmarks (AC: 1, 3)
  - [ ] Review `test_performance_k8s.py::test_event_publishing_throughput` implementation
  - [ ] Verify it tests with 10,000 events in batches of 100
  - [ ] Confirm concurrent publishing with asyncio.gather
  - [ ] Check JetStream integration is tested
  - [ ] Run the event benchmark: `pytest tests/integration/test_performance_k8s.py::TestPerformanceBenchmarksK8s::test_event_publishing_throughput -v`
  - [ ] Verify throughput meets minimum threshold (>5,000 events/s for port-forwarded)
- [ ] Task 4: Validate automated performance test runner (AC: 1)
  - [ ] Review `run_performance_benchmarks.py` implementation
  - [ ] Verify it includes warmup phase for RPC tests
  - [ ] Confirm it calculates all required statistics (mean, p50, p95, p99)
  - [ ] Check baseline comparisons are implemented
  - [ ] Run the automated script: `cd packages/aegis-sdk && python run_performance_benchmarks.py`
  - [ ] Verify it generates `performance_benchmark_final_report.md`
- [ ] Task 5: Execute comprehensive benchmarks against K8s NATS (AC: 2, 3)
  - [ ] Ensure K8s NATS cluster is running: `kubectl get pods -n aegis-trader | grep nats`
  - [ ] Verify port-forwarding is active for NATS on port 4222
  - [ ] Run full test suite: `pytest tests/integration/test_performance_k8s.py -v`
  - [ ] Document actual performance metrics:
    - RPC p99 latency: _____ms (target: <5ms with port-forwarding)
    - Event throughput: _____events/s (target: >5,000 events/s)
    - Memory per service: _____MB (target: <80MB)
  - [ ] Compare results with Story 1.1 baseline (3.330ms RPC, 5,403 events/s)
- [ ] Task 6: Generate and review performance report (AC: 2, 3)
  - [ ] Run test report generation: `pytest tests/integration/test_performance_k8s.py::TestPerformanceBenchmarksK8s::test_generate_performance_report -v`
  - [ ] Verify `performance_k8s_report.md` is created in packages/aegis-sdk/
  - [ ] Review report includes all required sections:
    - Executive summary with pass/fail status
    - Detailed RPC latency metrics (p50, p95, p99)
    - Event throughput measurements
    - Memory usage analysis
    - Test environment details
    - Recommendations for optimization
  - [ ] Ensure all acceptance criteria are documented as met
  - [ ] Create summary of any performance improvements since Story 1.1

## Dev Notes

### Previous Story Insights
From Story 1.1 implementation:
- Performance benchmarks were conducted during SDK validation
- Initial results showed RPC latency of 3.330ms (p99) with port-forwarded K8s connection
- Event throughput achieved 5,403 events/s (exceeds minimum viable threshold)
- Memory usage was minimal per service instance
- Test infrastructure already exists in `packages/aegis-sdk/tests/integration/`
- Port-forwarding script available: `packages/aegis-sdk/setup-nats-port-forward.sh`

### Epic 0 K8s Infrastructure Context
Based on Epic 0 implementation [Source: Story 0.2]:
- **NATS Cluster Configuration**:
  - 3-node HA cluster deployed via Helm charts
  - JetStream enabled with 10Gi FileStore PVC
  - Service name: `{{ .Release.Name }}-nats` on port 4222
  - GOMEMLIMIT set to 7GiB for 8Gi pods
  - Pod topology spread constraints for distribution
- **Access Method**:
  - Use kubectl port-forward for local testing: `kubectl port-forward svc/aegis-trader-nats 4222:4222`
  - ClusterIP service requires port-forwarding for external access
- **Namespace**: Deployments use configurable namespace (e.g., `aegis-test`)

### Performance Requirements
[Source: `packages/aegis-sdk/README.md`]:
- **RPC Pattern**:
  - Target latency: <1ms (p99) for local calls
  - Queue group load balancing support
  - Timeout handling (default 5s)
- **Event Pattern**:
  - Target throughput: 50,000+ events/s
  - JetStream for reliability
  - At-least-once delivery guarantee

### Technical Stack
[Source: `architecture/tech-stack.md`]:
- Python 3.13+ required
- NATS & JetStream 2.9+ as core messaging platform
- Testing framework: Pytest >=7.0.0
- All tests must use `@pytest.mark.asyncio` for async operations
- Use `@pytest.mark.parametrize` for multiple test cases

### Project Structure
[Source: `architecture/source-tree.md`]:
```
/packages/aegis-sdk/
├── tests/
│   ├── integration/        # Existing integration tests
│   └── performance/        # New performance benchmarks (to be created)
├── aegis_sdk/
│   ├── domain/
│   ├── application/
│   ├── ports/
│   └── infrastructure/
```

### Testing Standards
[Source: `architecture/testing-strategy.md`]:
- Test framework: **Pytest** (mandatory)
- Use fixtures instead of setUp/tearDown
- Use plain `assert` statements
- Test naming: `test_{functionality}_{expected_behavior}`
- Use testcontainers for NATS integration testing
- Minimum 80% coverage, 100% for critical paths

### Important Implementation Note
**CRITICAL**: Performance testing infrastructure already exists! This story focuses on **validation and documentation** of existing implementation rather than creating new code. The Dev Agent should:
1. Verify all performance test files exist and are properly implemented
2. Execute the tests against the K8s NATS cluster
3. Document the actual performance metrics achieved
4. Compare results with Story 1.1 baseline
5. Generate comprehensive performance reports

### Special Considerations
- **Existing Implementation**: The performance benchmarks were already created during Story 1.1. This story ensures they meet all requirements and documents official baselines.
- **"ultrathink" Note**: The user mentioned "ultrathink" in the activation args. This term was not found in the codebase. If encountered during review, note it for future clarification.
- **Integration with K8s**: All performance tests must be executed against the Kubernetes-deployed NATS cluster (already running in aegis-trader namespace)
- **Port-forwarding Overhead**: Performance measurements should account for kubectl port-forward latency when testing against K8s cluster. Expected overhead: ~2-3ms additional latency.

### Testing

**Test Execution**:
- Run from `/packages/aegis-sdk/` directory
- Performance tests: `pytest performance/ -v --benchmark-only`
- Generate HTML report: `pytest performance/ --benchmark-autosave --benchmark-save-data`

**Performance Test Organization**:
```
performance/
├── conftest.py              # Shared fixtures for performance tests
├── test_rpc_performance.py  # RPC pattern benchmarks
├── test_event_performance.py # Event pattern benchmarks
├── run_performance_benchmarks.py # Automated test runner
└── results/                 # Benchmark results storage
```

**Key Metrics to Capture**:
1. **RPC Pattern**:
   - Request-response latency (p50, p95, p99)
   - Maximum throughput (requests/second)
   - Queue group distribution efficiency
   - Serialization format impact
2. **Event Pattern**:
   - Publish latency
   - Maximum throughput (events/second)
   - JetStream acknowledgment overhead
   - Memory usage under load

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-02 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
